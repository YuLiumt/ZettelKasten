# Bayesian Information Criterion (BIC)

We have to balance the maximum likelihood of our model, $L$, against the number of model parameters, $k$. We seek the model with the fewest number of parameters that still does a good job explaining the data.
$$
\mathrm{BIC}=k \ln (n)-2 \ln (\widehat{L})
$$
The BIC balances the number of model parameters $k$ and number of data points $n$ against the maximum likelihood function, $L$. We seek to find the number of model parameters $k$ that minimizes the BIC.

You can always find a model that will fit your data, but that does not make it a great model. We should always choose the model that makes the fewest assumptions.

BIC gives us a way to choose between two different models with different numbers of parameters by selecting the one which gives us the lowest BIC score.

BIC can give us some clue as to whether the differences between models are meaningful. For instance, define $\Delta=B I C\left(M_{1} \mid D\right)-B I C\left(M_{2} \mid D\right)$. If $\Delta$ is positive, then $M_{2}$ is better than $M_{1}$ but how much better? Roughly speaking,
$$
|\Delta|=\left\{\begin{array}{ll}
0-1 & \text { insignificant } \\
1-3 & \text { meaningful } \\
3-5 & \text { strong } \\
5+ & \text { very strong }
\end{array}\right.
$$